<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <title>DEBM Python Package Manual</title>
    <meta name="author" content="Ofir Yakobi">
    <meta name="keywords" content="DEBM">
  </head>
  <body>
    <h2><span style="font-family: monospace;">DEBM is an open-source Python
        package for modeling of behavior in decision-from-experience tasks.</span></h2>
    <p><span style="font-family: monospace;"><a href="https://github.com/ofiryakobi/debm"
          target="_blank">GitHub </a>(<a href="https://github.com/ofiryakobi/debm/blob/main/README.md"
          target="_blank">README</a>)<br>
      </span></p>
    <hr style="color: white;">
    <p><br>
    </p>
    <h1>A description of objects and methods</h1>
    <p>Refer to the <a href="https://github.com/ofiryakobi/debm/blob/main/README.md"
        target="_blank">readme </a>file for a quick start with examples and use
      cases.</p>
    <p><br>
    </p>
    <p><em>A few illustrations of the basic built-in plotting functions</em></p>
    <p><img src="https://raw.githubusercontent.com/ofiryakobi/debm/main/img/fit.png"
        alt="plot_fit" style="width: 343px; height: 257px;"><img src="https://github.com/ofiryakobi/debm/raw/main/img/outcomes.png"
        alt="Prospect.outcome" style="width: 315px; height: 258px;"><img src="https://github.com/ofiryakobi/debm/raw/main/img/res_optimization_2d.png"
        alt="Optimization_plot" style="width: 346px; height: 259px;"><img src="https://github.com/ofiryakobi/debm/raw/main/img/res_optimization_3d.JPG"
        alt="3D figure of the loss function" style="width: 298px; height: 247px;"></p>
    <br>
    <p><br>
    </p>
    <hr>
    <h2>Prospect(trials, fun, oneOutcomePerRun, *args, **kwargs)</h2>
    <p>Construct a prospect in a decision-making problem.</p>
    <p><em>trials</em> - an integer defining the number of rounds in the choice
      problem.</p>
    <p><em>oneOutcomePerRun - </em>Boolean, if True - when generating the
      outcomes, the function runs <em>trials</em> number of times. If False,
      the function should return an array of outcomes in the size of <em>trials</em>.</p>
    <p><em>*args, **kwargs </em>- Arguments to be passed to <em>fun</em>.</p>
    <p><u>Example</u><strong>:</strong> </p>
    <p><br>
      Use NumPy's random.choice to generate 100 outcomes: each outcome is 3 (in
      probability 0.45), or otherwise zero.</p>
    <p><code>A=Prospect(100,np.random.choice,False,[3,0],100,True,[0.45,0.55])</code></p>
    <p><br>
    </p>
    <h3>Prospect.Generate()</h3>
    <p>Generate new outcomes (stored internally).</p>
    <h3>Prospect.EV(EVsims=10000)</h3>
    <p>Calculate the expected value, based on <em>EVsims </em>simulations.</p>
    <h3>Prospect.plot(blocks=None,nsim=1000)</h3>
    <p>Plot the outcomes based on <em>nsim </em>simulations.<br>
      If blocks is None - plot all trials; Else, aggregate over <em>blocks</em>
      blocks.</p>
    <p>Returns: fig, ax </p>
    <p></p>
    <h3>Operators</h3>
    <p>A&gt;B - the proportion of times the outcomes of A are greater than the
      outcomes of B, based on 1000 simulations (works with '&lt;' as well).</p>
    <p>A==B - the proportion of times the outcomes of A equal the outcomes of B,
      based on 1000 simulations.</p>
    <p><br>
    </p>
    <p></p>
    <p></p>
    <hr>
    <p><br>
    </p>
    <h2>Model(parameters, prospects, nsim, FullFeedback=True)</h2>
    <p>Construct a model object which includes the environment (the
      decision-making problem formulates in <em>prospects</em>, the behavior of
      the agents and their <em>parameters</em>), the number of simulations
      required for making predictions, and whether the decision-task<br>
      is full or partial feedback.</p>
    <p><em>parameters</em> - a dictionary (preferred) or a list of parameters'
      values. For instance {'Lambda': 0.1}. The value could be None (if no
      predictions are needed) or numeric.</p>
    <p><em>prospects</em> - a list of pre-defined Prospect objects. For example
      [StatusQuo, RiskyOption, Neutral].</p>
    <p><em>nsim</em> - an integer representing the number of simulations needed
      in every run of <em>Predict</em>.</p>
    <p><em>FullFeedback</em> - Boolean. Sets the decision-problem feedback scope
      (False == feedback for the chosen prospect only; True == feedback for all
      prospects). Note that not all models support partial feedback, hence the
      default <em>True</em> value.</p>
    <p>Specific types of models may add additional arguments prior to <em>parameters</em>,
      refer to the documentation of the specific model you are using.</p>
    <p><br>
    </p>
    <h3>Model.set_obs_choices(oc)</h3>
    <p>Store the observed choices internally (mandatory for plotting model's fit
      or estimating the model's parameters).</p>
    <p><em>oc</em> - an array with a shape of <em>trials </em>x <em>prospects
      </em>(i.e., each prospect is a column). Values should range 0 to 1,
      inclusive.</p>
    <p><br>
    </p>
    <h3>Model.get_predictions() </h3>
    <p>Returns the currently stored predictions. Could be None if no predictions
      were made (i.e., Model.Predict() was not run, and the model was not fitted
      to data).</p>
    <p>Returns an array with a shape of <em>trials </em>x <em>prospects.</em></p>
    <p><em><br>
      </em></p>
    <h3>Model.save_predictions(fname)</h3>
    <p>Save currently stored predictions into a csv file.</p>
    <p><em>fname </em>- file name to save the data to (<strong>note: </strong>will
      override the file).</p>
    <p><br>
    </p>
    <h3>Model.loss(loss="MSE",scope="prospectwise")</h3>
    <p>Returns the value of the loss function using the stored parameters,
      predictions, and observations.</p>
    <p><em>loss</em> - the loss function. Either MSE (a.k.a MSD) or LL (minus
      log-likelihood).</p>
    <p><em>scope</em> - the scope of the loss calculation. If prospectwise (or
      pw), will use the aggregated choices (i.e., mean choice rates over
      trials).<br>
      If <em>bitwise </em>(or bw) will use the trial level data.</p>
    <p>Returns a float.</p>
    <p><br>
    </p>
    <h3>Model.plot_predicted(blocks=None, **args)</h3>
    <p>Plots the stored predicted values in trials or blocks. Returns matplotlib
      objects (fig, axes) for further processing of the figure.</p>
    <p><em>blocks</em> - if None (default), will plot the predictions over
      trials. If an integer, will split the data into blocks of size
      trials/blocks.</p>
    <p><em>**args</em> - pass keyword arguments into pyplot.plot.</p>
    <p>Returns: Fig, Axes.</p>
    <p><br>
    </p>
    <h3>Model.plot_observed(blocks=None, **args)</h3>
    <p>Plots the stored observed values in trials or blocks. Returns matplotlib
      objects (fig, axes) for further processing of the figure. </p>
    <p><em>blocks</em> - if None (default), will plot the observations over
      trials. If an integer, will split the data into blocks of size
      trials/blocks.</p>
    <p><em>**args</em> - pass keyword arguments into pyplot.plot.</p>
    <p>Returns: Fig, Axes.</p>
    <p><br>
    </p>
    <h3>Model.plot_fit(blocks=None, **args)</h3>
    <p>Plots both observed and predicted values in trials or blocks. Returns
      matplotlib objects (fig, axes) for further processing of the figure. </p>
    <p><em>blocks</em> - if None (default), will plot the values over trials. If
      an integer, will split the data into blocks of size trials/blocks.</p>
    <p><em>**args</em> - pass keyword arguments into pyplot.plot.</p>
    <p>Returns: Fig, Axes.</p>
    <p><br>
    </p>
    <h3>Model.CalcLoss(parameters, *args, **kwargs)</h3>
    <p>Similar to Model.loss, only that parameters are passed and
      Model.Predict() is run before returning the loss.<br>
      Useful for using external optimization algorithms that pass parameters as
      arguments and expect a loss value in return.</p>
    <p> </p>
    <p><em>parameters</em> - a dictionary or a list of parameters' values. For
      instance {'Lambda': 0.1}. The value could be None (if no predictions are
      needed) or numeric.<br>
      The list should be ordered similarly to the order once the model was
      initiated.</p>
    <p><em>&nbsp;*args</em>- arguments to be passed to the <em>loss&nbsp; </em>function.
      <br>If two arguments are passed, they represent the <em>loss </em>and <em>scope</em>
      parameters (see Model.loss). <br>
      If three arguments are passed, the first represents the reGenerate
      argument (boolean) in Model.Predict, and the other two the <em>loss </em>and
      <em>scope</em> parameters (see Model.loss). </p>
    <p><em> **kwargs </em>- keyword arguments to be passed to the <em>loss&nbsp;
        </em>function.</p>
    <p><em></em>Example: Model.CalcLoss({'Alpha':2.3}, 'MSE', 'pw').</p>
    <p>Returns a float representing the loss function at the particular point in
      <em>parameters</em>.</p>
    <h3>Model.OptimizeBF(pars_dicts, pb=False, *args, **kwargs)</h3>
    <p>An exhaustive grid search (defined by the user). Returns the best set of
      parameters that were searched.<br>
      Slow, but works with all types of models.</p>
    <p><em>pars_dicts </em>- a list of dictionaries, where each dictionary
      contains values for all parameters in the model. For example:</p>
    <p>[{'Alpha':0.1, 'Lambda':0.1}, {'Alpha':0.1, 'Lambda':0.2}, {'Alpha':0.1,
      'Lambda':0.3}, {'Alpha':0.2, 'Lambda':0.1},....]</p>
    <p><em> pb -&nbsp; </em>show a progress bar (True/False)</p>
    <p><em>*args - </em>arguments to be passed to CalcLoss.</p>
    <p><em> **kwargs </em>- keyword arguments to be passed to CalcLoss.</p>
    <p>Returns a dictionary of results, with the following keys:</p>
    <p><em>bestp </em>- a dictionary with the best set of parameters found.</p>
    <p><em>losses </em>- an array of the loss value for each set of parameters.</p>
    <p><em>minloss </em>- the value of the min loss.</p>
    <p><em>parameters</em>_<em>checked </em>- equals pars_dict</p>
    <p><br>
    </p>
    <p><u>Tip</u>: use <em>makeGrid </em>to create <em>pars_dicts</em>.</p>
    <p>Example:</p>
    <p>pspace=makeGrid({'Alpha':range(0,101), 'Lambda':np.arange(0,1,0.001})</p>
    <p>results=myModel.OptimizeBF(pspace, True,False,'MSE','pw')</p>
    <h3><br>
    </h3>
    <h3>Model.Predict(reGenerate=True)</h3>
    <p></p>
    <p>This function is specified by the specific model, and generates the
      predictions of the model.</p>
    <p>It accepts reGenerate as an input, which determines if the outcomes of
      each prospect are re-generated in each simulation.</p>
    <p>Returns an array of predictions with a shape of <em>trials </em>x <em>prospects.</em>These
      predictions are also internally stored for further usage (e.g., plotting
      or fitting).</p>
    <hr>
    <p><br>
    </p>
    <h2>FitMultiGame(models,obs_choices)</h2>
    <p>This object allows fitting over multiple models and observations, for
      examples when agents complete more than one decision-making problem.</p>
    <p><em>models</em> - a list of models. All models should be of the same type
      because the same set of parameters is passed to every model in the list.</p>
    <p><em>obs_choices</em> - a list of matrices where each matrix is in the
      shape of <em>trials </em>x <em>prospects, </em>containing the observed
      choices.</p>
    <p><br>
    </p>
    <h3>FitMultiGame.CalcLoss(parameters, *args, **kwargs)</h3>
    Calculate the loss over all models specified for a given set of parameters.
    <p>See Model.CalcLoss for the description of arguments.</p>
    <br>
    <h3>FitMultiGame.OptimizeBF(pars_dicts, pb=False, *args, **kwargs)</h3>
    <p>Fit over all models. For a detailed description of the arguments and
      returned value, see Model.OptimizeBF.</p>
    <hr>
    <h2>Helper functions</h2>
    <h2><br>
    </h2>
    <h3>makeGrid(pars_dict)</h3>
    <p>Make a grid for grid search (OptimizeBF).</p>
    <p><em>pars_dict</em> - a dictionary of parameters with iterables.</p>
    <p>Returns: a list of dictionaries with a combination of all parameters'
      values sets.</p>
    <p><u>Example 1:</u></p>
    <p>grid=makeGrid({'Alpha':range(1,11)})</p>
    <p>print(grid)</p>
    <dl>
      <dd>[{'Alpha': 1},<br>
        &nbsp;{'Alpha': 2},<br>
        &nbsp;{'Alpha': 3},<br>
        &nbsp;{'Alpha': 4},<br>
        &nbsp;{'Alpha': 5},<br>
        &nbsp;{'Alpha': 6},<br>
        &nbsp;{'Alpha': 7},<br>
        &nbsp;{'Alpha': 8},<br>
        &nbsp;{'Alpha': 9},<br>
        &nbsp;{'Alpha': 10}]</dd>
    </dl>
    <p><u>Example 2:</u></p>
    <p>grid=makeGrid({'Alpha':[1,2,3],'Lambda':[0.1,0.5,0.9]})</p>
    <p>print(grid) </p>
    <dl>
      <dd>[{'Alpha': 1, 'Lambda': 0.1},<br>
        &nbsp;{'Alpha': 1, 'Lambda': 0.5},<br>
        &nbsp;{'Alpha': 1, 'Lambda': 0.9},<br>
        &nbsp;{'Alpha': 2, 'Lambda': 0.1},<br>
        &nbsp;{'Alpha': 2, 'Lambda': 0.5},<br>
        &nbsp;{'Alpha': 2, 'Lambda': 0.9},<br>
        &nbsp;{'Alpha': 3, 'Lambda': 0.1},<br>
        &nbsp;{'Alpha': 3, 'Lambda': 0.5},<br>
        &nbsp;{'Alpha': 3, 'Lambda': 0.9}]</dd>
    </dl>
    <p><u>Example 3:</u> </p>
    <p>grid=makeGrid({'Alpha':np.arange(1,20,6),'Lambda':np.arange(0,1,0.3)})</p>
    <p>print(grid)</p>
    <dl>
      <dd>[{'Alpha': 1, 'Lambda': 0.0},<br>
        &nbsp;{'Alpha': 1, 'Lambda': 0.3},<br>
        &nbsp;{'Alpha': 1, 'Lambda': 0.6},<br>
        &nbsp;{'Alpha': 1, 'Lambda': 0.9},<br>
        &nbsp;{'Alpha': 7, 'Lambda': 0.0},<br>
        &nbsp;{'Alpha': 7, 'Lambda': 0.3},<br>
        &nbsp;{'Alpha': 7, 'Lambda': 0.6},<br>
        &nbsp;{'Alpha': 7, 'Lambda': 0.9},<br>
        &nbsp;{'Alpha': 13, 'Lambda': 0.0},<br>
        &nbsp;{'Alpha': 13, 'Lambda': 0.3},<br>
        &nbsp;{'Alpha': 13, 'Lambda': 0.6},<br>
        &nbsp;{'Alpha': 13, 'Lambda': 0.9},<br>
        &nbsp;{'Alpha': 19, 'Lambda': 0.0},<br>
        &nbsp;{'Alpha': 19, 'Lambda': 0.3},<br>
        &nbsp;{'Alpha': 19, 'Lambda': 0.6},<br>
        &nbsp;{'Alpha': 19, 'Lambda': 0.9}]</dd>
    </dl>
    <p><br>
    </p>
    <p></p>
    <h3>saveEstimation(res,fname,**kwargs)</h3>
    <p>Save estimation results to a csv file.</p>
    <p><em>res</em> - a results dictionary generated by OptimizeBF.</p>
    <p><em>fname </em>- the name of the file to be saved (<strong>note</strong>:
      overrides an existing file).</p>
    <p><em>**kwargs </em>- optional keyword arguments to be passed to
      Pandas.to_csv</p>
    <h2><br>
    </h2>
    <h3>resPlot(res)</h3>
    <p>Plot the results of a <em>results </em>dictionary (generated by
      OptimizeBF)</p>
    <p>In case of a one-parameter model, will plot a 2D figure.</p>
    <p>In case of a two-parameter model, will plot a 3D figure.</p>
    <p>In other cases, the X axis will show the iteration number (less useful).</p>
    <p>Returns: fig, ax (1/2 parameters) or fig, ax, surf (3 parameters).</p>
    <p><br>
    </p>
    <hr>
    <h2>Creating your own model</h2>
    <p>The following is a template that one could use:&nbsp; <br>
      <br>
      class yourModeName(Model): <br>
      &nbsp;&nbsp;&nbsp; def __init__(self,*args):<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Model.__init__(self,*args)<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.name="Give your model a
      name"<br>
      &nbsp;&nbsp;&nbsp; def Predict(self, reGenerate=True):<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if type(self.parameters)==dict:<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      self._x=self.parameters['X'] # Read parameters (dict style)<br>
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      self._y=self.parameters['Y']<br>
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; # change the
      naming of the parameters add more if needed<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; elif
      type(self.parameters)==list:<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      self._x=self.parameters[0] # Read parameters (list style)<br>
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      self._y=self.parameters[1]<br>
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; # add more if
      needed<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else:<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; raise
      Exception("Parameters should be a list or a
      dictionary")&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      grand_choices=np.zeros((self.nsim,self._trials_,self._Num_of_prospects_))
      #this would be an array of predicted choices<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for s in range(self.nsim):<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if
      reGenerate:<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      for p in self.prospects:<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      p.Generate()<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      data=np.vstack([x.outcomes for x in self.prospects]).transpose() #This is
      an array containing the outcomes from all prospects (shaped <em>trials </em>x
      <em>prospects)</em></p>
    <p><em></em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      for i in range(self._trials_): # In this template we iterate over trials<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      if i==0: # Assume random choice in the first trial<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      choice=np.random.choice(self._Num_of_prospects_)<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      grand_choices[s,i,choice]=1<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      else:<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      # This is the main "brain" of your model<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      # Stating what happens in every trial <br>
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; # (after the first random choice)<br>
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; # i is the trial index (starting
      from 0)<br>
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; # s is the simulation index.<br>
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; # grand_choices is the matrix where
      you store choices<br>
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; # You should set the chosen choice
      to 1 in each trial (or a value between 0 to 1 if your model is built this
      way).<br>
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; # For instance, if s=5, i=10, and
      c=2,<br>
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; # grand_choices[s,i,c]=1 means that
      in the 6th<br>
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; # simulation (indexing in Python
      start from 0),<br>
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; # on the 11th trial, the 3rd choice
      is chosen.<br>
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; # Row 11 in the 6th simulation will
      look like that:<br>
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; # 0 0 1 (assuming there are three
      prospects).<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      self._pred_choices_=np.mean(grand_choices,axis=0) # Average accross
      simulations,<br>
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; #Store internally in
      self._pred_choices_<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return self._pred_choices_ #
      Return the predicted choices<br>
      ```<br>
      <br>
      Thus, the minimal steps for creating your own model involve&nbsp; <br>
      setting up a name, the parameters, and writing the main algorithm (the
      "brain" of your model).&nbsp; </p>
    <hr>
    <p><br>
    </p>
    <h2>A list of built-in models</h2>
    <h3>FullData()</h3>
    <p>Use all available historical feedback in each trial.</p>
    <p><br>
    </p>
    <h3>NaiveSampler(kappa_or_less)</h3>
    <p>A K sized sample is drawn with replacement in each trial (after a first
      random choice), and a prospect is chosen according to the best average of
      the sample.<br>
      In each simulation a new K is drawn from a uniform distribution between 1
      to Kappa.<br>
      Additionally, if kappa_or_less==True, in each trial the sample size is
      drawn from 1 to K (thus, the sample size changes from trial to trial).</p>
    <p>Arguments: kappa_or_less (Boolean), see explanation above.</p>
    <p>Parameters: Kappa (integer, 1 to the number of trials).</p>
    <h3><br>
    </h3>
    <h3>Sample_of_K</h3>
    <p>Sample of K is essentially the Naive Sampler, with kappa_or_less set to
      False.</p>
    <p>Parameters: Kappa (integer, 1 to the number of trials).</p>
    <p><br>
    </p>
    <h3>NaiveSampler_2S</h3>
    <p>Two-stage Naive Sampler. In the first stage, decide between the risky
      choices. In the second stage, decide between<br>
      the chosen risky choice and the rest of the choices.</p>
    <h3>ISAW2</h3>
    <h3>SAW</h3>
    <h3>RL_delta_rule</h3>
    <h3>Population_1or99</h3>
    <h3>Population_1or3f</h3>
    <p><br>
    </p>
    <p></p>
    <p></p>
    <p><br>
    </p>
    <p></p>
  </body>
</html>
